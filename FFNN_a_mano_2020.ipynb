{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN_a_mano_2020",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilich917/data-science-ipython-notebooks/blob/master/FFNN_a_mano_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr5EsNZxJhN8"
      },
      "source": [
        "# CC6204 Deep Learning, Universidad de Chile\n",
        "## Código de Red Neuronal \"a mano\"\n",
        "El siguiente código muestra de manera tan directa como fue posible la forma de crear una red neuronal Feed Forward de dos capas escondidas usando solo las funcionalidades para operar tensores de pytorch (clases y funciones dentro del módulo [`torch`](http://pytorch.org/docs/master/torch.html#module-torch)) más las clases `torch.nn.Module` y `torc.nn.Parameter`.\n",
        "\n",
        "La idea del código es mostrar cómo:\n",
        "\n",
        "*   crear todos los parámetros de una red Feed Forward usando tensores,\n",
        "*   computar la pasada hacia adelante de la red (predicción/forward) usando funciones sobre los mismos tensores y los datos de entrada por paquetes/batches (usando funciones de sigmoid y tangente hiperbólica como activación, y sigmoid final para el outpu),\n",
        "*   computar la función de pérdida para la red (entropía cruzada en este caso),\n",
        "*   calcular los gradientes desde la función de pérdida para todos los parámetros usando operadores sobre los tensores con el método de Back Propagation (backward), \n",
        "*   actualizar los parámetros usando descenso de gradiente, y\n",
        "*   reportar las métricas de predicción de la red sobre los datos.\n",
        "\n",
        "Todos los pasos anteriores se repiten en un loop de entrenamiento por tantas iteraciones como se quiera (epochs). Otro punto muy importante es que gracias a pytorch y CoLaboratory, podemos realizar todas las pruebas de manera muy simple en una GPU. El código también sirve para explorar el impacto de realizar estos entrenamientos con hardware especializado.\n",
        "\n",
        "Una de las gracias de usar pytorch es que nos permite hacer todo lo anterior a mano, paso a paso, sólo utilizando funciones básicas. Esto se podría también haber realizado utilizando sólo [NumPy](http://www.numpy.org/)  (es un buen ejercicio, hágalo!), pero no podríamos usar la GPU tan fácilmente como en pytorch.\n",
        "\n",
        "El código está pensado para acompañar la clase de Back Propagation de CC6204 y servir como una introducción rápida a las funcionalidades básicas de pytorch. No está pensado en ser un código modular, si no más bien un código pedagógico para los temas de grafo de computación, back propagation, y descenso de gradiente por paquetes. Para entender el código requiere haber calculado antes derivadas de la función de pérdida (con lapiz y papel).\n",
        "\n",
        "(Pensado para correr en [Colaboratory](http://colab.research.google.com))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AZ0MHy17fOv"
      },
      "source": [
        "Código por Jorge Pérez\n",
        "\n",
        "https://github.com/jorgeperezrojas\n",
        "\n",
        "[@perez](https://twitter.com/perez)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JVQ13l32QpB"
      },
      "source": [
        "# Esto es solo para poder debugear.\n",
        "# !pip install -q ipdb\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "# import ipdb\n",
        "\n",
        "# Genera una semilla fija para que los experimentos sea repetibles.\n",
        "t_cg = torch.manual_seed(1547)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_EdIfcNA49D"
      },
      "source": [
        "# Función sigmoid, recibe un objeto torch.Tensor\n",
        "def sig(T):\n",
        "  return torch.reciprocal(1 + torch.exp(-1 * T))\n",
        "\n",
        "# Función tanh, recibe un objeto torch.Tensor\n",
        "def tanh(T):\n",
        "  E = torch.exp(T)\n",
        "  e = torch.exp(-1 * T)\n",
        "  return (E - e) * torch.reciprocal(E + e)\n",
        "\n",
        "# Función de pérdida\n",
        "def bi_cross_ent_loss(y_pred, y, safe=True, epsilon=1e-7):\n",
        "  # tamaño del batch\n",
        "  N = y.size()[0]\n",
        "  \n",
        "  if safe:\n",
        "    # Asegura que no haya valores indefinidos.\n",
        "    y_pred = y_pred.clamp(epsilon, 1-epsilon)\n",
        "  \n",
        "  B = (1-y) * torch.log(1-y_pred) + y * torch.log(y_pred)\n",
        "  return -1/N * torch.sum(B)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW5MjPReZLhY"
      },
      "source": [
        "# Las redes en pytorch deben heredar desde torch.nn.Module\n",
        "class FFNN(torch.nn.Module):\n",
        "  def __init__(self, d0=300, d1=200, d2=300, init_v=1):\n",
        "    super(FFNN, self).__init__()\n",
        "    \n",
        "    # Crea los tensores como parámetros\n",
        "    self.W1 = torch.nn.Parameter(torch.randn(d0,d1) * init_v) \n",
        "    self.b1 = torch.nn.Parameter(torch.zeros(d1))\n",
        "    self.W2 = torch.nn.Parameter(torch.randn(d1,d2) * init_v)\n",
        "    self.b2 = torch.nn.Parameter(torch.zeros(d2))\n",
        "    self.U = torch.nn.Parameter(torch.randn(d2,1) * init_v)\n",
        "    self.c = torch.nn.Parameter(torch.zeros(1))\n",
        "      \n",
        "  # Computa la pasada hacia adelante\n",
        "  def forward(self, x):\n",
        "  \n",
        "    u1 = x @ self.W1 + self.b1\n",
        "    h1 = tanh(u1)\n",
        "    u2 = h1 @ self.W2 + self.b2\n",
        "    h2 = sig(u2)\n",
        "    u3 = h2 @ self.U + self.c\n",
        "    y_pred = sig(u3)\n",
        "    \n",
        "    self._cache = [u1, u2] # podemos usar un mejor cache?\n",
        "    \n",
        "    return y_pred\n",
        "  \n",
        "  # Backpropagation \n",
        "  def backward(self, x, y, y_pred):\n",
        "    # recuperamos el \"cache\"\n",
        "    u1, u2 = self._cache # podemos usar un mejor cache?\n",
        "\n",
        "    # tamaño del batch\n",
        "    b = x.size()[0]\n",
        "    \n",
        "    # Estas son derivadas calculadas a mano\n",
        "    dL_du3 = (1/b) * (y_pred - y)\n",
        "    dL_dU  = sig(u2).t() @ dL_du3\n",
        "    dL_dc  = torch.sum(dL_du3, 0)\n",
        "    dL_dh2 = dL_du3 @ self.U.t()\n",
        "    dL_du2 = dL_dh2 * sig(u2) * (1 - sig(u2)) \n",
        "    dL_dW2 = tanh(u1).t() @ dL_du2\n",
        "    dL_db2 = torch.sum(dL_du2, 0)\n",
        "    dL_dh1 = dL_du2 @ self.W2.t()\n",
        "    dL_du1 = dL_dh1 * (1 - tanh(u1) * tanh(u1))\n",
        "    dL_dW1 = x.t() @ dL_du1\n",
        "    dL_db1 = torch.sum(dL_du1,0)\n",
        "        \n",
        "    # Registra los valores de gradientes en cada tensor (que nos interesa)\n",
        "    grads = [dL_dW1, dL_db1, dL_dW2, dL_db2, dL_dU, dL_dc]\n",
        "    params = [self.W1, self.b1, self.W2, self.b2, self.U, self.c]\n",
        "    for p, g in zip(params, grads):\n",
        "      p.grad = g \n",
        "      \n",
        "  def num_parameters(self):\n",
        "    total = 0\n",
        "    for p in self.parameters():\n",
        "      total += p.numel()\n",
        "    return total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui-y_i-UR1Rz"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RandomDataSet(Dataset):\n",
        "  def __init__(self, N, f):\n",
        "    R_N_f = torch.rand(N,f)\n",
        "    self.X = torch.bernoulli(R_N_f)\n",
        "    R_N_1 = torch.rand(N,1)\n",
        "    self.Y = torch.bernoulli(R_N_1)\n",
        "    self.num_features = f\n",
        "    \n",
        "  # Debemos definir __len__ para retornar el tamaño del dataset\n",
        "  def __len__(self):\n",
        "    return self.X.size()[0]\n",
        "\n",
        "  # Debemos definir __getitem__ para retornar el i-ésimo \n",
        "  # ejemplo en nuestro dataset.\n",
        "  def __getitem__(self, i):\n",
        "    return self.X[i], self.Y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh96-3s-ZjSL"
      },
      "source": [
        "def loop_FFNN(dataset, batch_size, d1, d2, lr, \n",
        "                 epochs, run_in_GPU=True, reports_every=1, \n",
        "                 cheq_grad=False, init_v=1):\n",
        "  \n",
        "  # Define un tipo para los tensores según si correrá en la GPU o no\n",
        "  device = 'cuda' if run_in_GPU else 'cpu'\n",
        "    \n",
        "  # d0 es la cantidad de features  \n",
        "  d0 = dataset.num_features\n",
        "  \n",
        "  # Crea la red\n",
        "  red = FFNN(d0, d1, d2, init_v)\n",
        "    \n",
        "  # Pasa la red al dispositivo elegido\n",
        "  red.to(device)\n",
        "        \n",
        "  # Muestra la cantidad de parámetros\n",
        "  print('Cantidad de parámetros:', red.num_parameters())\n",
        "  \n",
        "  # Crea un dataloader desde el dataset\n",
        "  data = DataLoader(dataset, batch_size, shuffle=True)\n",
        "  \n",
        "  # Comienza el entrenamiento\n",
        "  tiempo_epochs = 0\n",
        "  for e in range(1, epochs+1):  \n",
        "    inicio_epoch = time.clock()\n",
        "    \n",
        "    for (x, y) in data:     \n",
        "      # Asegura de pasarlos a la GPU si fuera necesario\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      \n",
        "      # Computa la pasada hacia adelante (forward)\n",
        "      y_pred = red.forward(x)\n",
        "      \n",
        "      # Computa la función de pérdida\n",
        "      L = bi_cross_ent_loss(y_pred, y)\n",
        "      \n",
        "      # Computa los gradientes hacia atrás (backpropagation)\n",
        "      red.backward(x, y, y_pred)\n",
        "      \n",
        "      # Descenso de gradiente para actualizar los parámetros\n",
        "      for p in red.parameters():\n",
        "        p.data -= lr * p.grad\n",
        "      \n",
        "    tiempo_epochs += time.clock() - inicio_epoch\n",
        "    \n",
        "    # Reporta el acierto cada \"reports_every\" cantidad de épocas\n",
        "    if e % reports_every == 0:\n",
        "      \n",
        "      # Calcula la certeza de las predicciones sobre todo el conjunto\n",
        "      X = dataset.X.to(device)\n",
        "      Y = dataset.Y.to(device)\n",
        "\n",
        "      # Predice usando la red\n",
        "      Y_PRED = red.forward(X)\n",
        "      \n",
        "      # Calcula la pérdida de todo el conjunto\n",
        "      L_total = bi_cross_ent_loss(Y_PRED, Y)\n",
        "\n",
        "      # Elige una clase dependiendo del valor de Y_PRED\n",
        "      Y_PRED_BIN = (Y_PRED >= 0.5).float()\n",
        "\n",
        "      correctos = torch.sum(Y_PRED_BIN == Y).item()\n",
        "      acc = (correctos / N) * 100\n",
        "\n",
        "      sys.stdout.write(\n",
        "            '\\rEpoch:{0:03d}'.format(e) + ' Acc:{0:.2f}%'.format(acc)\n",
        "            + ' Loss:{0:.4f}'.format(L_total) \n",
        "            + ' Tiempo/epoch:{0:.3f}s'.format(tiempo_epochs/e))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJx5kndFBhgi"
      },
      "source": [
        "N = 5000 # numero de ejemplos\n",
        "f = 300 # numero de features\n",
        "\n",
        "dataset = RandomDataSet(N,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgaASdM0FnJ9"
      },
      "source": [
        "epochs = 20\n",
        "loop_FFNN(dataset, batch_size=10, d1=300, d2=400, epochs=epochs, \n",
        "             run_in_GPU=True, lr=0.06, init_v=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oluz-gTqF09b"
      },
      "source": [
        "Para ejemplificar, probar el anterior código de la siguiente forma:\n",
        "\n",
        "*   Cambiar el tamaño del batch desde 2 a 1000: visualizar tiempo de entrenamiento vs Acc\n",
        "*   Cambiar el tamaño de las capas / número de parámetros (llegar a 1000/2000) y visualizar tiempo de entrenamiento (qué pasa si entrenas en la CPU en vez de la GPU?)\n",
        "*   Cambiar el valor máximo de inicialización (0.01,1,1.5,2)\n",
        "*   Mostrar que se pueden calcular (aun) más eficiente la pasada hacia atrás reutilizando algunos valores previamente computados cuando derivamos `sig` y `tanh`: visualizar el tiempo de entrenamiento.\n",
        "*   Mostrar como `.backward()` puede hacer todo el trabajo de backpropagation por nosotros. Ojo con anular los gradientes con `.zero_()` y mantener la red de tamaño razonable por la estabilidad numérica (200/300 tamaño parece bien)\n"
      ]
    }
  ]
}